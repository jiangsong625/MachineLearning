{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "导入包  \n",
    "TensorFlow Datasets 是 TensorFlow 生态系统中的一个核心组件，它旨在简化数据获取和预处理流程，为机器学习和深度学习项目提供即用型数据集。TFDS 提供了一系列高质量、标准化的数据集，支持多种数据类型，包括图像、文本、视频等，下面是其主要特点和使用方法的概览："
   ],
   "id": "c6d843d47d42bceb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:27:28.315658Z",
     "start_time": "2024-06-09T11:26:54.288219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras.callbacks\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers"
   ],
   "id": "9f226fd0a2b984de",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:33:55.479175Z",
     "start_time": "2024-06-09T11:33:55.471163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EPOCHS = 10  # 训练轮数\n",
    "BATCH_SIZE = 128  # 批处理大小\n",
    "VERBOSE = 1  # 是否打印日志\n",
    "OPTIMIZER = optimizers.Adam()  # 优化器\n",
    "VALIDATION_SPLIT = 0.95  # 是指训练集的划分比例，0.95表示训练集95%，验证集5%"
   ],
   "id": "2314cbca2c7fbefc",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:28:39.927777Z",
     "start_time": "2024-06-09T11:28:39.909818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "IMG_ROWS, IMG_COLS = 28, 28  # 输入图片的维度\n",
    "INPUT_SHAPE = (IMG_ROWS, IMG_COLS, 1)\n",
    "NB_CLASSES = 10  # 输出的类别数"
   ],
   "id": "c5c0e4d0fa0cf6d4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "定义LeNET网络",
   "id": "f844ad0c029e8639"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:28:42.047409Z",
     "start_time": "2024-06-09T11:28:42.034445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_model(input_shape, classes):\n",
    "    model = models.Sequential()\n",
    "    # CONV => RELU => POOL\n",
    "    model.add(layers.Convolution2D(20 ,(5, 5), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    # CONV => RELU => POOL\n",
    "    model.add(layers.Convolution2D(50, (5, 5), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    # Flatten => RELU layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(500, activation='relu'))\n",
    "    # a softmax classifier\n",
    "    model.add(layers.Dense(classes, activation='softmax'))\n",
    "    return model"
   ],
   "id": "b8d16924f0e8b65c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### `build_model` 函数解析\n",
    "\n",
    "该函数运用 Keras（TensorFlow 库的构成部分）构建了一个序列化的卷积神经网络（CNN）模型，广泛应用于图像分类任务。下文详述模型结构：\n",
    "\n",
    "- **输入层**  \n",
    "  通过`input_shape`参数隐式设定，确保与输入图像尺寸（不含颜色通道）相符。\n",
    "\n",
    "- **卷积层 1**  \n",
    "  添加了20个5x5大小的滤波器，每个滤波器后接ReLU激活函数，负责提取图像基础特征。\n",
    "\n",
    "- **最大池化 1**  \n",
    "  采用2x2大小的最大池化减少空间复杂度，增进特征的平移不变性。\n",
    "\n",
    "- **卷积层 2**  \n",
    "  增加至50个5x5滤波器，深化特征学习，提取更高级特征。\n",
    "\n",
    "- **最大池化 2**  \n",
    "  继续通过2x2最大池化降低维度，简化数据结构。\n",
    "\n",
    "- **展平层**  \n",
    "  将卷积层输出的二维特征图转化为一维向量，适配全连接层。\n",
    "\n",
    "- **全连接层（Dense 1）**  \n",
    "  含500个神经元，使用ReLU激活，整合并强化特征。\n",
    "\n",
    "- **输出层（Dense 2）**  \n",
    "  神经元数等于分类数，采用softmax激活，输出各类别概率，实现多分类。\n",
    "\n",
    "- **模型概览**  \n",
    "  显示模型结构摘要，包括参数总量，帮助评估模型复杂度。\n",
    "\n",
    "- **返回模型**  \n",
    "  构建完毕的模型待编译与训练。\n",
    "\n",
    "此架构作为图像分类的基本CNN模型，为众多项目奠定基础。然而，现代实践倾向于加入批量归一化、dropout机制或Inception模块等，以提升性能和模型的泛化能力。\n"
   ],
   "id": "1325f5e36cf336f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "加载数据集",
   "id": "57005e7cb83b0bec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:28:48.549930Z",
     "start_time": "2024-06-09T11:28:47.884832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "# 重塑\n",
    "X_train = x_train.reshape(x_train.shape[0], IMG_ROWS, IMG_COLS, 1)\n",
    "X_test = x_test.reshape(x_test.shape[0], IMG_ROWS, IMG_COLS, 1)\n",
    "# 归一化\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "# \n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# 转换为one-hot编码\n",
    "Y_train = tf.keras.utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(y_test, NB_CLASSES)"
   ],
   "id": "f9c0f767715ecb75",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9252d2d3e187d669"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:29:00.197268Z",
     "start_time": "2024-06-09T11:28:59.098605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = build_model(input_shape=INPUT_SHAPE, classes=NB_CLASSES)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ],
   "id": "ff9a213300d668fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 24, 24, 20)        520       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 12, 12, 20)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 50)          25050     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 4, 4, 50)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 800)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               400500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 431,080\n",
      "Trainable params: 431,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:32:20.232055Z",
     "start_time": "2024-06-09T11:32:20.228964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./LeNet_logs')\n",
    "]"
   ],
   "id": "533a0a507b91248e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "eb164722a4d0e930"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:35:03.784653Z",
     "start_time": "2024-06-09T11:34:17.378037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=VERBOSE,\n",
    "                    validation_split=VALIDATION_SPLIT,\n",
    "                    callbacks=callbacks)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)"
   ],
   "id": "59dae617ca4d1153",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 [==============================] - 5s 224ms/step - loss: 0.0992 - accuracy: 0.9743 - val_loss: 0.1596 - val_accuracy: 0.9507\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 4s 190ms/step - loss: 0.0777 - accuracy: 0.9783 - val_loss: 0.1462 - val_accuracy: 0.9547\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - 4s 187ms/step - loss: 0.0690 - accuracy: 0.9833 - val_loss: 0.1376 - val_accuracy: 0.9580\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - 4s 189ms/step - loss: 0.0549 - accuracy: 0.9850 - val_loss: 0.1347 - val_accuracy: 0.9587\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 5s 193ms/step - loss: 0.0417 - accuracy: 0.9893 - val_loss: 0.1197 - val_accuracy: 0.9631\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 4s 190ms/step - loss: 0.0368 - accuracy: 0.9903 - val_loss: 0.1327 - val_accuracy: 0.9588\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 4s 187ms/step - loss: 0.0232 - accuracy: 0.9957 - val_loss: 0.1231 - val_accuracy: 0.9640\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 4s 187ms/step - loss: 0.0242 - accuracy: 0.9943 - val_loss: 0.1249 - val_accuracy: 0.9636\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 4s 188ms/step - loss: 0.0144 - accuracy: 0.9987 - val_loss: 0.1118 - val_accuracy: 0.9680\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 4s 189ms/step - loss: 0.0127 - accuracy: 0.9970 - val_loss: 0.1250 - val_accuracy: 0.9654\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.0919 - accuracy: 0.9720\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2ed4ee4517c92900"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:35:19.586871Z",
     "start_time": "2024-06-09T11:35:19.576897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ],
   "id": "13e5f3a3d7133745",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.09191161394119263\n",
      "Test accuracy: 0.972000002861023\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d5977f7e4e0b3a48"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
