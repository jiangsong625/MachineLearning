有监督学习：

- 回归算法
- 分类

无监督算法

- 聚类分析





# 1 线性回归模型

## 1.1 单变量线性回归

$$
f_{m,y}(x)=wx+b\\
f(x)=wx+b
$$



### 1.1.1 代价函数

平方误差代价函数(m=样本数 )
$$
J(w,b)=\frac{1}{2m}*\sum_{1=1}^m(\hat{y}^{(i)}-y^{(i)})^2\\
J(w,b)=\frac{1}{2m}*\sum_{1=1}^m(f_{w,b}(x^{(i)})-y^{(i)})^2
$$



## 1.2 梯度下降法

### 1.2.1 概念

1. 步长（Learning rate，又称 学习率）
2. 特征（feature，即样本输入值）
3. 假设函数（hypothesis function）
4. 损失函数（loss function）

$$
J(w,b)=\frac{1}{2m}*\sum_{1=1}^m(\hat{y}^{(i)}-y^{(i)})^2
$$

### 1.2.2 公式

$$
w=w-\alpha\frac{\partial}{\partial w}J(w,b)\\
b=b-\alpha\frac{\part}{\part b}J(w,b)
$$

 

&part;是偏导符号求$J(w,b)$关于$w$的偏导

&alpha;是学习率，又叫学习步长 
$$
w=w-\alpha \frac{1}{m}\sum_{i=1}^m(h_w(x^{(i)})-y^{(i)})x^{(i)}\\
b=b-\alpha \frac{1}{m}\sum_{i=1}^m(f_{w,b}(x^{(i)})-y^{(i)})
$$

梯度下降法核心代码

```python
# 批量梯度下降法
def gradient_descent(X, Y, theta, alpha, iters ):  # alpha是学习率，iters是迭代次数
    temp = np.matrix(np.zeros(theta.shape))  # 然后将temp变为矩阵[0.,0.]，保留迭代参数的中间值
    parameters = int(theta.ravel().shape[1])  # 参数数量，ravel()的作用是将多维数组变为一维数组
    cost = np.zeros(iters)
    for i in range(iters):
        error = (X * theta.T) - Y  # 预测值与样本的差

        for j in range(parameters):
            term = np.multiply(error, X[:, j])  # term是偏导数
            temp[0, j] = theta[0, j] - ((alpha / len(X)) * np.sum(term))  # 更新theta

        theta = temp
        cost[i] = compute_cost(X, Y, theta)

    return theta, cost
```

代价函数

```python
# 定义代价函数
def compute_cost(X, Y, theta):
    inner = np.power((X * theta.T) - Y, 2)
    return np.sum(inner) / (2 * len(X))
```








